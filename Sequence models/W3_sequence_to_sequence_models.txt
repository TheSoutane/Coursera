## Notes of the Sequence models course provided by DeepLearning.ai ##

Week 3: Sequence to sequence models
	Basic models
		encoder-decoder structure
			decoder: output word per word and use last word plus hiddent state to predict the next one
		
		image captioning
			Use convolution layers then flatten it and then use a generator to create text
	
	Most likely sentence selection
		RNN for translation:
			encoder decoder architecture
			Computing P(output|Input)
			select most likely
	Beam search (for decoder)
		define B: Beam width
			number of 1st words to be explored (in, I, this for ex)
		Get the B most likely first words from the decoder
			Then select the B most likely sequences accross the potential 2uplets based on the B sampled 1st words
				If the sampled 2 uplets are In the, in my and this big, I is discarded
				The beam name comes from the stable number of sequence considered
		Beam search refinement	
			Length Normalization
				The use of log to compute probabilitye (instead of product) helps stabilizing computation
			Other issue: decreasing probability with sentence length (due to combined probability)
				-> Normalize by thanslation length (divide by T_y, length of the output
			Overall faster but no guarantee of optimal solution
		Beam search error analysis
			goal: define if error comes rom beam search or model
				Compute Bsearch expected output proba and compare it to beam search output
	Bleu score
		differenciate between potentially good answers in translation
		def: input and ref outputs
			get credit per word ~ appearances in ref/appearances in the output
			then per bigram (ordered group of 2 words)
			combine scores together (sum)
				1 means perfect score
	Attention model intuition
		Issue: with current model, there is a need to synthetize the complete sentence
			Limits performance for long inputs
		Attention is a way of focusing on the part of the input that matters
		Mechanism:
			Use a forward and backward pass 
			Translate based on context
				context: sum  of weighted intermediary state
	Speech recognition
		Problem definition
			Audio clip x -> text y
			x: ai pressure through time
				pre-proc into 3d plot (spectrogram)
			y: usually built with phonemes (basic unit of sound)
				End to end deep learning replaced it
				Data size usually from 300 to 3000 h, best systems 100.000h
			Architecture
				Due to input high freq (100Hz for ex), output might return the same character multiple times then collapses it
					ex: ttt_h_eee___ ___qqq__ for the q
				Attention model ?
				Cost: CTC cost
		Trigger word detection
			trigger: ex "Hey Siri"
			RNN returning 1 when it detects the word and 0 otherwise
			