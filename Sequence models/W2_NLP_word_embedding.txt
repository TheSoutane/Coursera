## Notes of the Sequence models course provided by DeepLearning.ai ##

Week 2: NLP and word embedding
	word embedding intro
		Previous working method: one hot encoding of vocaabulary
			Limitation: limited cross words understanding (as scalar product is =0)
		Alternative: featurized representation of words with relevant scalar product (ex man*woman = -1, apple*orange=1,...)
			Significantly reduces the number of dimensions
		Application of features to NLP
			better generalisation of insights
				similar words are grouped together which facilitates generalisation
		Properties		
			semantic geometry (man-wonam ~king-queen)
		Embedding Matrix
	Practical word embedding
		word prediction: use embedding of the k last words to predict next word
			Can also include next k' words
			outputs embedding (or OHE) of the predicted word
		Word2Vect
			skip grams
				input: context word (random word)
				target: word K times before or after in the sentence
				Model struct: input->embedding->softmax->predict
				Loss: distance between the pred and the real target
			Limitations:
				computational speed softmax implies sum over 10000 elements (vocab size)
				solution: Hierachical softmax
					is in which 5k->2.5k->... words
						retain the binding only
						can use variable depth to speed up algo
		Negative sampling
			similar to skip gram but more efficient
			structure:
				context and word selection is the same
				Turn problem into binary classification
					input: context + another word
					Target: if another word is word: return 1 else 0
						do it for k negative answers
				named negative sampling as we undersample the pb
	GloVe (global vector for Word representation):
		X_ij: # of times word j appears in the context of i
		Model: sum minimize(y_hat-log(X_ij))**2 for X_ij >0
		Good to understand context
	Applications using word embedding
		Sentiment classification
			input: sentence, text
			output: rank, note
			process:
				one hot encoding through dict
				apply embedding
				options:
					Apply softmax classifier
						There is a limit to the acceptable size
						No consideration in word order
					Apply RNN predictor
						Many to one structure
	De biasing word embedding
		Reflect bias in the corpus
		Solved applying
			define dircetion (avg(man-women))
				eq to principal component
			Neutralize bias: add projection to get rid of bias wherever it appears
			Equalize pairs
				maque equidistant to neutral elements (ex: granfather, grandmother and child care)
				